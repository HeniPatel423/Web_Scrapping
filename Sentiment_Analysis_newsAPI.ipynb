{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5c042a",
   "metadata": {},
   "source": [
    "# News Articles Seniment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df23ccb",
   "metadata": {},
   "source": [
    "# Importing Librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f699a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "\n",
    "# linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# data Processing\n",
    "import pandas as pd\n",
    "\n",
    "# data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32757a88",
   "metadata": {},
   "source": [
    "# Loading Data into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45111f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Description</th>\n",
       "      <th>Published At</th>\n",
       "      <th>Title</th>\n",
       "      <th>Source ID</th>\n",
       "      <th>Source Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When Emma Vaughan left for work after turning ...</td>\n",
       "      <td>It is the third local house fire in two weeks ...</td>\n",
       "      <td>2024-03-09T12:38:26Z</td>\n",
       "      <td>Bride-to-be 'devastated' after tumble dryer fire</td>\n",
       "      <td>bbc-news</td>\n",
       "      <td>BBC News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An AI tool tested by an NHS hospital trust suc...</td>\n",
       "      <td>An AI tool called Mia found missed breast canc...</td>\n",
       "      <td>2024-03-21T01:16:08Z</td>\n",
       "      <td>NHS AI test spots tiny cancers missed by doctors</td>\n",
       "      <td>bbc-news</td>\n",
       "      <td>BBC News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Join Fox News for access to this content\\r\\nPl...</td>\n",
       "      <td>Apple's Journal app may make some of your pers...</td>\n",
       "      <td>2024-03-06T11:00:15Z</td>\n",
       "      <td>The iPhone privacy setting you need to turn off</td>\n",
       "      <td>fox-news</td>\n",
       "      <td>Fox News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Join Fox News for access to this content\\r\\nPl...</td>\n",
       "      <td>A stealthy technology known as EM Eye allows e...</td>\n",
       "      <td>2024-03-21T14:00:21Z</td>\n",
       "      <td>Creepy tool lets criminal hackers access your ...</td>\n",
       "      <td>fox-news</td>\n",
       "      <td>Fox News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Join Fox News for access to this content\\r\\nPl...</td>\n",
       "      <td>Tech guru Kurt \"CyberGuy\" Knutsson reveals the...</td>\n",
       "      <td>2024-03-20T14:00:15Z</td>\n",
       "      <td>The 4 best secret note-taking apps that can ch...</td>\n",
       "      <td>fox-news</td>\n",
       "      <td>Fox News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  When Emma Vaughan left for work after turning ...   \n",
       "1  An AI tool tested by an NHS hospital trust suc...   \n",
       "2  Join Fox News for access to this content\\r\\nPl...   \n",
       "3  Join Fox News for access to this content\\r\\nPl...   \n",
       "4  Join Fox News for access to this content\\r\\nPl...   \n",
       "\n",
       "                                         Description          Published At  \\\n",
       "0  It is the third local house fire in two weeks ...  2024-03-09T12:38:26Z   \n",
       "1  An AI tool called Mia found missed breast canc...  2024-03-21T01:16:08Z   \n",
       "2  Apple's Journal app may make some of your pers...  2024-03-06T11:00:15Z   \n",
       "3  A stealthy technology known as EM Eye allows e...  2024-03-21T14:00:21Z   \n",
       "4  Tech guru Kurt \"CyberGuy\" Knutsson reveals the...  2024-03-20T14:00:15Z   \n",
       "\n",
       "                                               Title Source ID Source Name  \n",
       "0   Bride-to-be 'devastated' after tumble dryer fire  bbc-news    BBC News  \n",
       "1   NHS AI test spots tiny cancers missed by doctors  bbc-news    BBC News  \n",
       "2    The iPhone privacy setting you need to turn off  fox-news    Fox News  \n",
       "3  Creepy tool lets criminal hackers access your ...  fox-news    Fox News  \n",
       "4  The 4 best secret note-taking apps that can ch...  fox-news    Fox News  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "df = pd.read_csv(r\"news_articles.csv\")\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8409964",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c468b6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "Content          0\n",
      "Description      0\n",
      "Published At     0\n",
      "Title            0\n",
      "Source ID       59\n",
      "Source Name      0\n",
      "dtype: int64\n",
      "Missing Value Ratio:\n",
      "Content         0.000000\n",
      "Description     0.000000\n",
      "Published At    0.000000\n",
      "Title           0.000000\n",
      "Source ID       0.241803\n",
      "Source Name     0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Print the count of missing values\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "\n",
    "print(missing_values)\n",
    "\n",
    "# Calculate the missing value ratio\n",
    "\n",
    "missing_ratio = df.isnull().mean()\n",
    "\n",
    "# Print the missing value ratio\n",
    "\n",
    "print(\"Missing Value Ratio:\")\n",
    "\n",
    "print(missing_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecba38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Raws which have null values\n",
    "# Did this step because the rest columns with missing values are the review related text and replacement of the text will not be accurate.\n",
    "df = df.dropna()\n",
    "\n",
    "#remove duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89336ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Value Ratio:\n",
      "Content         0.0\n",
      "Description     0.0\n",
      "Published At    0.0\n",
      "Title           0.0\n",
      "Source ID       0.0\n",
      "Source Name     0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the missing value ratio\n",
    "\n",
    "missing_ratio = df.isnull().mean()\n",
    "\n",
    "# Print the missing value ratio\n",
    "\n",
    "print(\"Missing Value Ratio:\")\n",
    "\n",
    "print(missing_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd245546",
   "metadata": {},
   "source": [
    "\n",
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a621286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Structure:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 149 entries, 0 to 230\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Content       149 non-null    object\n",
      " 1   Description   149 non-null    object\n",
      " 2   Published At  149 non-null    object\n",
      " 3   Title         149 non-null    object\n",
      " 4   Source ID     149 non-null    object\n",
      " 5   Source Name   149 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 8.1+ KB\n",
      "None\n",
      "-------\n",
      "\n",
      "\n",
      "Data Variables:\n",
      "Index(['Content', 'Description', 'Published At', 'Title', 'Source ID',\n",
      "       'Source Name'],\n",
      "      dtype='object')\n",
      "-------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#examine the structure of the data\n",
    "print(\"Data Structure:\")\n",
    "print(df.info())\n",
    "print(\"-------\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Data Variables:\")\n",
    "print(df.columns)\n",
    "print(\"-------\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6de96",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9847f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Distributions:\n",
      "                                                  Content  \\\n",
      "count                                                 149   \n",
      "unique                                                148   \n",
      "top     Join Fox News for access to this content\\r\\nPl...   \n",
      "freq                                                    2   \n",
      "\n",
      "                                              Description  \\\n",
      "count                                                 149   \n",
      "unique                                                144   \n",
      "top     Stay up to date on the latest AI technology ad...   \n",
      "freq                                                    6   \n",
      "\n",
      "                Published At  \\\n",
      "count                    149   \n",
      "unique                   149   \n",
      "top     2024-03-09T12:38:26Z   \n",
      "freq                       1   \n",
      "\n",
      "                                                   Title Source ID Source Name  \n",
      "count                                                149       149         149  \n",
      "unique                                               149         5           5  \n",
      "top     Bride-to-be 'devastated' after tumble dryer fire  bbc-news    BBC News  \n",
      "freq                                                   1        73          73  \n",
      "\n",
      "Data Objects:\n",
      "                                                  Content  \\\n",
      "count                                                 149   \n",
      "unique                                                148   \n",
      "top     Join Fox News for access to this content\\r\\nPl...   \n",
      "freq                                                    2   \n",
      "\n",
      "                                              Description  \\\n",
      "count                                                 149   \n",
      "unique                                                144   \n",
      "top     Stay up to date on the latest AI technology ad...   \n",
      "freq                                                    6   \n",
      "\n",
      "                Published At  \\\n",
      "count                    149   \n",
      "unique                   149   \n",
      "top     2024-03-09T12:38:26Z   \n",
      "freq                       1   \n",
      "\n",
      "                                                   Title Source ID Source Name  \n",
      "count                                                149       149         149  \n",
      "unique                                               149         5           5  \n",
      "top     Bride-to-be 'devastated' after tumble dryer fire  bbc-news    BBC News  \n",
      "freq                                                   1        73          73  \n"
     ]
    }
   ],
   "source": [
    "# Print data distributions\n",
    "print(\"\\nData Distributions:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Print data distributions for object columns\n",
    "print(\"\\nData Objects:\")\n",
    "print(df.describe(include=['object']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6821b",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44422b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing special characters and punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Removing numerical values\n",
    "    tokens = [word for word in tokens if not word.isdigit()]\n",
    "\n",
    "    # Removing extra whitespaces\n",
    "    tokens = [word.strip() for word in tokens if word.strip()]\n",
    "\n",
    "    # Joining tokens back to a single string\n",
    "    processed_text = \" \".join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Applying text preprocessing to the 'Review' column of the DataFrame\n",
    "df['Content'] = df['Content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a44f8",
   "metadata": {},
   "source": [
    "# Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ed1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    sentiment = sentiment_scores['compound']\n",
    "\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    word_scores = []\n",
    "    for token in tokens:\n",
    "        word_score = sid.polarity_scores(token)\n",
    "        word_scores.append((token, word_score))\n",
    "    #print(\"word scores:\", word_scores)\n",
    "    \n",
    "    if sentiment >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "    \n",
    "# Applying sentiment analysis to the 'Review' column of the DataFrame\n",
    "df['Sentiment_token'] = df['Content'].apply(analyze_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "874fbb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Description</th>\n",
       "      <th>Published At</th>\n",
       "      <th>Title</th>\n",
       "      <th>Source ID</th>\n",
       "      <th>Source Name</th>\n",
       "      <th>Sentiment_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emma vaughan left work turning tumble dryer id...</td>\n",
       "      <td>It is the third local house fire in two weeks ...</td>\n",
       "      <td>2024-03-09T12:38:26Z</td>\n",
       "      <td>Bride-to-be 'devastated' after tumble dryer fire</td>\n",
       "      <td>bbc-news</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ai tool tested nhs hospital trust successfully...</td>\n",
       "      <td>An AI tool called Mia found missed breast canc...</td>\n",
       "      <td>2024-03-21T01:16:08Z</td>\n",
       "      <td>NHS AI test spots tiny cancers missed by doctors</td>\n",
       "      <td>bbc-news</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>join fox news access content plus special acce...</td>\n",
       "      <td>Apple's Journal app may make some of your pers...</td>\n",
       "      <td>2024-03-06T11:00:15Z</td>\n",
       "      <td>The iPhone privacy setting you need to turn off</td>\n",
       "      <td>fox-news</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>join fox news access content plus special acce...</td>\n",
       "      <td>A stealthy technology known as EM Eye allows e...</td>\n",
       "      <td>2024-03-21T14:00:21Z</td>\n",
       "      <td>Creepy tool lets criminal hackers access your ...</td>\n",
       "      <td>fox-news</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>join fox news access content plus special acce...</td>\n",
       "      <td>Tech guru Kurt \"CyberGuy\" Knutsson reveals the...</td>\n",
       "      <td>2024-03-20T14:00:15Z</td>\n",
       "      <td>The 4 best secret note-taking apps that can ch...</td>\n",
       "      <td>fox-news</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  emma vaughan left work turning tumble dryer id...   \n",
       "1  ai tool tested nhs hospital trust successfully...   \n",
       "2  join fox news access content plus special acce...   \n",
       "3  join fox news access content plus special acce...   \n",
       "4  join fox news access content plus special acce...   \n",
       "\n",
       "                                         Description          Published At  \\\n",
       "0  It is the third local house fire in two weeks ...  2024-03-09T12:38:26Z   \n",
       "1  An AI tool called Mia found missed breast canc...  2024-03-21T01:16:08Z   \n",
       "2  Apple's Journal app may make some of your pers...  2024-03-06T11:00:15Z   \n",
       "3  A stealthy technology known as EM Eye allows e...  2024-03-21T14:00:21Z   \n",
       "4  Tech guru Kurt \"CyberGuy\" Knutsson reveals the...  2024-03-20T14:00:15Z   \n",
       "\n",
       "                                               Title Source ID Source Name  \\\n",
       "0   Bride-to-be 'devastated' after tumble dryer fire  bbc-news    BBC News   \n",
       "1   NHS AI test spots tiny cancers missed by doctors  bbc-news    BBC News   \n",
       "2    The iPhone privacy setting you need to turn off  fox-news    Fox News   \n",
       "3  Creepy tool lets criminal hackers access your ...  fox-news    Fox News   \n",
       "4  The 4 best secret note-taking apps that can ch...  fox-news    Fox News   \n",
       "\n",
       "  Sentiment_token  \n",
       "0        Negative  \n",
       "1        Negative  \n",
       "2        Positive  \n",
       "3        Positive  \n",
       "4        Positive  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c86b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_excel('Sentiment_Articles.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be78afe0",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7164d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5848505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df['Content']\n",
    "y = df['Sentiment_token']\n",
    "\n",
    "\n",
    "#convert Text data into Numerical feature using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3eac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0086e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7ccb694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, lr_predictions)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03099cd2",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f199481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, nb_predictions)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d89bb",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f84d795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, rf_predictions)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
